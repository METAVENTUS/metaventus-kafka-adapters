# Metaventus Kafka Adapters

Bienvenue dans **Metaventus Kafka Adapters** â€“ un **monorepo** Go qui fournit tous les Ã©lÃ©ments nÃ©cessaires pour interagir avec un **cluster Kafka** hÃ©bergÃ© sur **Confluent Cloud**. Lâ€™objectif est de proposer des **clients Producer et Consumer** robustes, des **modÃ¨les Avro** clairement dÃ©finis et un **mini-CLI** en Go pour gÃ©rer la configuration Confluent (topics, schÃ©mas, etc.).

---

## ğŸ“Œ Sommaire

1. [Structure gÃ©nÃ©rale du projet](#1-structure-gÃ©nÃ©rale-du-projet)
2. [FonctionnalitÃ©s](#2-fonctionnalitÃ©s)
3. [Configuration & Fichier `.env`](#3-configuration--fichier-env)
4. [Modules principaux](#4-modules-principaux)
5. [Exemple de consommation/production Kafka](#5-exemple-de-consommationproduction-kafka)
6. [Exemple de gestion Confluent Cloud via CLI](#6-exemple-de-gestion-confluent-cloud-via-cli)
7. [Tests et intÃ©gration continue](#7-tests-et-intÃ©gration-continue)
8. [DÃ©pendances et outils utilisÃ©s](#8-dÃ©pendances-et-outils-utilisÃ©s)

---

## 1. Structure gÃ©nÃ©rale du projet

```
metaventus-kafka-adapters/
â”‚â”€â”€ avro_kafka_config/         # Gestion de Confluent Cloud (topics, schÃ©mas) + CLI
â”‚   â”œâ”€â”€ config.go              # Chargement de la config (via .env)
â”‚   â”œâ”€â”€ client.go              # MÃ©thodes pour crÃ©er/lister les topics
â”‚   â”œâ”€â”€ schema_registry.go     # Enregistrement des schÃ©mas Avro auprÃ¨s du Schema Registry
â”‚   â”œâ”€â”€ README.md              # Documentation spÃ©cifique au module avro_kafka_config
â”‚
â”‚â”€â”€ avro_schemas/              # Centralisation des schÃ©mas Avro
â”‚   â”œâ”€â”€ avro_schemas.go        # Map { nomDuSchÃ©ma : JSON du schÃ©ma }
â”‚   â”œâ”€â”€ schemas/               # Dossier contenant les fichiers de schÃ©mas Avro
â”‚
â”‚â”€â”€ cmd/                       # CLI pour exÃ©cuter des commandes (ex: crÃ©er un topic, etc.)
â”‚   â”œâ”€â”€ main.go                # Programme principal pour gÃ©rer Confluent Cloud
â”‚   â”œâ”€â”€ .env                   # Fichier de configuration locale (Non commitÃ©)
â”‚
â”‚â”€â”€ consumer/                  # Package pour implÃ©menter un Consumer Kafka gÃ©nÃ©rique
â”‚   â”œâ”€â”€ config.go              # Config Consumer (brokers, group ID, nombre de workers...)
â”‚   â”œâ”€â”€ consumer.go            # ImplÃ©mentation d'un Consumer gÃ©nÃ©rique (gÃ©nÃ©riques Go)
â”‚
â”‚â”€â”€ producer/                  # Package pour implÃ©menter un Producer Kafka gÃ©nÃ©rique
â”‚   â”œâ”€â”€ config.go              # Config Producer (brokers, authentification SASL/TLS...)
â”‚   â”œâ”€â”€ producer.go            # ImplÃ©mentation d'un Producer gÃ©nÃ©rique
â”‚
â”‚â”€â”€ models/                    # ModÃ¨les Go correspondant aux schÃ©mas Avro
â”‚
â”‚â”€â”€ go.mod                     # Module Go principal
â”‚â”€â”€ README.md                  # Documentation gÃ©nÃ©rale du repository
```

---

## 2. FonctionnalitÃ©s

- **Producer/Consumer Kafka** :
   - Structuration claire (config, encodage Avro, clÃ©s de partition, etc.).
   - PossibilitÃ© de consommer plusieurs messages en parallÃ¨le (multi-workers).
   - Gestion de lâ€™authentification SASL/TLS pour Confluent Cloud.
   - **Filtrage optionnel des messages en fonction des horaires d'ouverture**.

- **Schemas Avro centralisÃ©s** :
   - Tous les schÃ©mas Avro sont stockÃ©s dans `avro_schemas/`.
   - Chaque modÃ¨le Go (`models/`) peut renvoyer son schÃ©ma via `GetSchema()`.
   - Compatible avec un usage direct du **Confluent Schema Registry**.

- **Gestion avancÃ©e du Consumer** :
   - VÃ©rification de la connexion Ã  Kafka au dÃ©marrage.
   - Gestion dynamique des **heures dâ€™ouverture** (peut Ãªtre activÃ©e via `IsBusinessHours`).
   - Support du **multi-workers** pour la consommation parallÃ¨le.

- **CLI pour Confluent Cloud** :
   - CrÃ©ation et listing de topics.
   - Enregistrement des schÃ©mas Avro.
   - Chargement automatique des credentials via `.env`.

---

## 3. Configuration & Fichier `.env`

Ajout d'un paramÃ¨tre optionnel pour gÃ©rer les **heures d'ouverture du Consumer** :

```ini
# Activer la gestion des heures d'ouverture (true/false)
KAFKA_CONSUMER_BUSINESS_HOURS=true
```

Si activÃ©, le consumer ne traitera que les messages entre **9h et 19h, du lundi au vendredi**.

---

## 4. Exemple de consommation avec gestion des horaires

```go
cfg := consumer.Config{
Brokers:        []string{"your-cluster.confluent.cloud:9092"},
Topic:          "example-topic",
GroupID:        "example-group",
NumWorkers:     3,
IsBusinessHours: true, // â† Activer la gestion des horaires
}

c := consumer.NewConsumer[models.ModelExample](cfg)

c.Consume(context.Background(), cfg, func(ctx context.Context, event models.ModelExample) error {
log.Printf("Message reÃ§u : %+v", event)
return nil
})
```

---

## 5. Exemple de gestion Confluent Cloud via CLI

1. **Lister les topics** :
   ```sh
go run cmd/main.go list-topics
```
2. **CrÃ©er un topic** :
   ```sh
go run cmd/main.go create-topic my-topic
```
3. **Enregistrer un schÃ©ma** :
   ```sh
go run cmd/main.go register-schema ExampleSchema
```

---

## 6. Tests et intÃ©gration continue

Ce projet inclut des tests d'intÃ©gration pour garantir le bon fonctionnement des **Consumers** et **Producers**.

### ğŸ” **Tests d'intÃ©gration avec TestContainers**
Nous utilisons **TestContainers** pour lancer un Kafka temporaire et tester nos composants en conditions rÃ©elles.

Exemple de test de consommation Kafka :

```go
func (s *ConsumerIntegrationSuite) Test_ConsumerReceivesMessage() {
cfg := consumer.Config{
Brokers:        []string{s.kafkaHostPort},
Topic:          s.topicName,
GroupID:        "test-group",
NumWorkers:     1,
IsBusinessHours: true,
}

c := consumer.NewConsumer[TestEvent](cfg)
msgCh := make(chan TestEvent, 1)

c.Start(s.ctx, func(ctx context.Context, event TestEvent) error {
msgCh <- event
return nil
})
defer func() { s.Require().NoError(c.Close()) }()

testMsg := TestEvent{ID: "123", Data: "Hello Kafka"}
p, err := producer.NewProducer(s.ctx, producer.Config{
Brokers: []string{s.kafkaHostPort},
Topic:   s.topicName,
})
s.Require().NoError(err)
s.Require().NoError(p.Publish(s.ctx, testMsg))

select {
case evt := <-msgCh:
s.Assert().Equal("123", evt.ID)
s.Assert().Equal("Hello Kafka", evt.Data)
case <-time.After(10 * time.Second):
s.Fail("Aucun message reÃ§u aprÃ¨s 10s")
}
}
```

### ğŸ”§ **ExÃ©cution des tests**
Pour exÃ©cuter les tests :
```sh
go test ./...
```

Ou pour afficher plus de dÃ©tails :
```sh
go test -v ./...
```

---

## 7. DÃ©pendances et outils utilisÃ©s

- **[segmentio/kafka-go](https://github.com/segmentio/kafka-go)**
- **[hamba/avro](https://github.com/hamba/avro)**
- **[testcontainers-go](https://github.com/testcontainers/testcontainers-go)**
- **API REST** de **Confluent Schema Registry**
- **Go 1.21+** pour les gÃ©nÃ©riques et amÃ©liorations de performance.

---

C'est Ã  jour avec toutes les nouvelles fonctionnalitÃ©s ! ğŸš€ Dis-moi si tu veux d'autres ajustements.